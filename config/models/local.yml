provider: local
default_model: "microsoft/phi-2"
models:
  microsoft/phi-2:
    context_window: 2048
    max_output_tokens: 2048
    capabilities:
      - streaming
      - temperature_control
      - stop_sequences
  meta-llama/Llama-2-7b-hf:
    context_window: 4096
    max_output_tokens: 4096
    capabilities:
      - streaming
      - temperature_control
      - stop_sequences
  mistralai/Mistral-7B-v0.1:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
      - streaming
      - temperature_control
      - stop_sequences
  EleutherAI/gpt-neo-1.3B:
    context_window: 2048
    max_output_tokens: 2048
    capabilities:
      - streaming
      - temperature_control
      - stop_sequences
  google/flan-t5-base:
    context_window: 512
    max_output_tokens: 512
    capabilities:
      - streaming
      - temperature_control
metadata:
  updated_at: '2025-05-29T17:00:00.000000'
  source: local_config