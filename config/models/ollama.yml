provider: ollama
default_model: ollama/hf.co/unsloth/Qwen3-8B-GGUF:IQ4_XS
models:
  ollama/hf.co/unsloth/Qwen3-8B-GGUF:IQ4_XS:
    context_window: 4096
    capabilities:
    - streaming
  ollama/hf.co/unsloth/Qwen3-8B-GGUF:IQ4_NL:
    context_window: 4096
    capabilities:
    - streaming
  ollama/hf.co/bartowski/Zyphra_ZR1-1.5B-GGUF:Q8_0:
    context_window: 4096
    capabilities:
    - streaming
  ollama/nomic-embed-text:latest:
    context_window: 4096
    capabilities:
    - streaming
  ollama/hf.co/mradermacher/Darkest-muse-v1-GGUF:Q8_0:
    context_window: 4096
    capabilities:
    - streaming
  ollama/codegemma:
    context_window: 8192
    max_output_tokens: 8192
    mode: completion
  ollama/codegeex4:
    context_window: 32768
    max_output_tokens: 8192
    capabilities:
    - streaming
  ollama/deepseek-coder-v2-instruct:
    context_window: 32768
    max_output_tokens: 8192
    capabilities:
    - streaming
    - function_calling
  ollama/deepseek-coder-v2-base:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - function_calling
    mode: completion
  ollama/deepseek-coder-v2-lite-instruct:
    context_window: 32768
    max_output_tokens: 8192
    capabilities:
    - streaming
    - function_calling
  ollama/deepseek-coder-v2-lite-base:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - function_calling
    mode: completion
  ollama/internlm2_5-20b-chat:
    context_window: 32768
    max_output_tokens: 8192
    capabilities:
    - streaming
    - function_calling
  ollama/llama2:
    context_window: 4096
    max_output_tokens: 4096
    capabilities:
    - streaming
  ollama/llama2:7b:
    context_window: 4096
    max_output_tokens: 4096
    capabilities:
    - streaming
  ollama/llama2:13b:
    context_window: 4096
    max_output_tokens: 4096
    capabilities:
    - streaming
  ollama/llama2:70b:
    context_window: 4096
    max_output_tokens: 4096
    capabilities:
    - streaming
  ollama/llama2-uncensored:
    context_window: 4096
    max_output_tokens: 4096
    mode: completion
  ollama/llama3:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - streaming
  ollama/llama3:8b:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - streaming
  ollama/llama3:70b:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - streaming
  ollama/llama3.1:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - streaming
    - function_calling
  ollama/mistral-large-instruct-2407:
    context_window: 65536
    max_output_tokens: 8192
    capabilities:
    - streaming
    - function_calling
  ollama/mistral:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - function_calling
    mode: completion
  ollama/mistral-7B-Instruct-v0.1:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - streaming
    - function_calling
  ollama/mistral-7B-Instruct-v0.2:
    context_window: 32768
    max_output_tokens: 32768
    capabilities:
    - streaming
    - function_calling
  ollama/mixtral-8x7B-Instruct-v0.1:
    context_window: 32768
    max_output_tokens: 32768
    capabilities:
    - streaming
    - function_calling
  ollama/mixtral-8x22B-Instruct-v0.1:
    context_window: 65536
    max_output_tokens: 65536
    capabilities:
    - streaming
    - function_calling
  ollama/codellama:
    context_window: 4096
    max_output_tokens: 4096
    mode: completion
  ollama/orca-mini:
    context_window: 4096
    max_output_tokens: 4096
    mode: completion
  ollama/vicuna:
    context_window: 2048
    max_output_tokens: 2048
    mode: completion
metadata:
  updated_at: '2025-06-06T14:30:54.820943'
  source: ollama_api
