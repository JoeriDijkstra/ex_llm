provider: ollama
default_model: llama2
models:
  llama2:
    context_window: 4096
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  llama2:7b:
    context_window: 4096
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  llama2:13b:
    context_window: 4096
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  llama2:70b:
    context_window: 4096
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  llama3:
    context_window: 8192
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  llama3:8b:
    context_window: 8192
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  llama3:70b:
    context_window: 8192
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  llama3.1:
    context_window: 128000
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  llama3.1:8b:
    context_window: 128000
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  llama3.1:70b:
    context_window: 128000
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  llama3.1:405b:
    context_window: 128000
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  codellama:
    context_window: 16384
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - code
    - streaming
  codellama:7b:
    context_window: 16384
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - code
    - streaming
  codellama:13b:
    context_window: 16384
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - code
    - streaming
  codellama:34b:
    context_window: 16384
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - code
    - streaming
  llava:
    context_window: 4096
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - vision
    - streaming
  llava:7b:
    context_window: 4096
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - vision
    - streaming
  llava:13b:
    context_window: 4096
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - vision
    - streaming
  llava:34b:
    context_window: 4096
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - vision
    - streaming
  mistral:
    context_window: 32768
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  mixtral:
    context_window: 32768
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  phi3:
    context_window: 128000
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  qwen2.5:
    context_window: 32768
    pricing:
      input: 0.0
      output: 0.0
    capabilities:
    - text
    - streaming
  ollama/codegemma:
    context_window: 8192
    max_output_tokens: 8192
  ollama/codegeex4:
    context_window: 32768
    max_output_tokens: 8192
    capabilities:
    - streaming
  ollama/deepseek-coder-v2-instruct:
    context_window: 32768
    max_output_tokens: 8192
    capabilities:
    - function_calling
    - streaming
  ollama/deepseek-coder-v2-base:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - function_calling
  ollama/deepseek-coder-v2-lite-instruct:
    context_window: 32768
    max_output_tokens: 8192
    capabilities:
    - function_calling
    - streaming
  ollama/deepseek-coder-v2-lite-base:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - function_calling
  ollama/internlm2_5-20b-chat:
    context_window: 32768
    max_output_tokens: 8192
    capabilities:
    - function_calling
    - streaming
  ollama/llama2:
    context_window: 4096
    max_output_tokens: 4096
    capabilities:
    - streaming
  ollama/llama2:7b:
    context_window: 4096
    max_output_tokens: 4096
    capabilities:
    - streaming
  ollama/llama2:13b:
    context_window: 4096
    max_output_tokens: 4096
    capabilities:
    - streaming
  ollama/llama2:70b:
    context_window: 4096
    max_output_tokens: 4096
    capabilities:
    - streaming
  ollama/llama2-uncensored:
    context_window: 4096
    max_output_tokens: 4096
  ollama/llama3:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - streaming
  ollama/llama3:8b:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - streaming
  ollama/llama3:70b:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - streaming
  ollama/llama3.1:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - function_calling
    - streaming
  ollama/mistral-large-instruct-2407:
    context_window: 65536
    max_output_tokens: 8192
    capabilities:
    - function_calling
    - streaming
  ollama/mistral:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - function_calling
  ollama/mistral-7B-Instruct-v0.1:
    context_window: 8192
    max_output_tokens: 8192
    capabilities:
    - function_calling
    - streaming
  ollama/mistral-7B-Instruct-v0.2:
    context_window: 32768
    max_output_tokens: 32768
    capabilities:
    - function_calling
    - streaming
  ollama/mixtral-8x7B-Instruct-v0.1:
    context_window: 32768
    max_output_tokens: 32768
    capabilities:
    - function_calling
    - streaming
  ollama/mixtral-8x22B-Instruct-v0.1:
    context_window: 65536
    max_output_tokens: 65536
    capabilities:
    - function_calling
    - streaming
  ollama/codellama:
    context_window: 4096
    max_output_tokens: 4096
  ollama/orca-mini:
    context_window: 4096
    max_output_tokens: 4096
  ollama/vicuna:
    context_window: 2048
    max_output_tokens: 2048
