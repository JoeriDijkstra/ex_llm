# ExLLM Test Environment Configuration
# 
# This file contains all environment variables needed to run the complete ExLLM test suite.
# Copy this file to .env.test and fill in your API keys to run integration tests.
#
# IMPORTANT: Never commit .env.test with real API keys!

# =============================================================================
# PROVIDER API KEYS
# =============================================================================

# Anthropic Claude
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# OpenAI GPT
OPENAI_API_KEY=your-openai-api-key-here

# Google Gemini
GEMINI_API_KEY=your-gemini-api-key-here
# Alternative: GOOGLE_API_KEY=your-google-api-key-here

# Groq
GROQ_API_KEY=your-groq-api-key-here

# Mistral
MISTRAL_API_KEY=your-mistral-api-key-here

# OpenRouter
OPENROUTER_API_KEY=your-openrouter-api-key-here

# Perplexity
PERPLEXITY_API_KEY=your-perplexity-api-key-here

# X.AI Grok
XAI_API_KEY=your-xai-api-key-here

# =============================================================================
# LOCAL SERVICES
# =============================================================================

# Ollama (local model server)
OLLAMA_HOST=http://localhost:11434

# LM Studio (local model server)
LMSTUDIO_HOST=http://localhost:1234

# =============================================================================
# OAUTH2 CONFIGURATION (for Gemini advanced APIs)
# =============================================================================

# Google OAuth2 (required for Gemini tuned models and corpus APIs)
GOOGLE_CLIENT_ID=your-google-client-id.apps.googleusercontent.com
GOOGLE_CLIENT_SECRET=your-google-client-secret

# =============================================================================
# TEST CONFIGURATION
# =============================================================================

# Enable test response caching (speeds up repeated test runs)
EX_LLM_TEST_CACHE_ENABLED=false

# Force live API calls (bypasses cache when set to true)
MIX_RUN_LIVE=false

# Log level for debugging (debug/info/warn/error/none)
EX_LLM_LOG_LEVEL=none

# =============================================================================
# OPTIONAL OVERRIDES
# =============================================================================

# Custom base URLs (for self-hosted or proxy endpoints)
# ANTHROPIC_BASE_URL=https://api.anthropic.com/v1
# OPENAI_BASE_URL=https://api.openai.com/v1
# GEMINI_BASE_URL=https://generativelanguage.googleapis.com
# GROQ_BASE_URL=https://api.groq.com/openai/v1
# MISTRAL_BASE_URL=https://api.mistral.ai/v1
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# PERPLEXITY_BASE_URL=https://api.perplexity.ai
# XAI_BASE_URL=https://api.x.ai

# Default models (override provider defaults)
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
# OPENAI_MODEL=gpt-4-turbo-preview
# GEMINI_MODEL=gemini-1.5-pro-latest
# GROQ_MODEL=llama-3.1-70b-versatile
# MISTRAL_MODEL=mistral-large-latest
# OPENROUTER_MODEL=openai/gpt-4o-mini
# PERPLEXITY_MODEL=llama-3.1-sonar-small-128k-online
# XAI_MODEL=grok-3
# OLLAMA_MODEL=llama3.2

# =============================================================================
# NOTES
# =============================================================================
#
# 1. To run tests with this configuration:
#    $ cp .env.test.example .env.test
#    $ # Edit .env.test and add your API keys
#    $ source .env.test && mix test --include integration
#
# 2. For CI/CD environments, set these as secrets in your pipeline
#
# 3. Some providers offer free tiers or trial keys:
#    - OpenRouter: Sign up at https://openrouter.ai
#    - Groq: Get free API key at https://console.groq.com
#    - Perplexity: Sign up at https://www.perplexity.ai/settings/api
#
# 4. Local services (Ollama, LM Studio) don't require API keys but must be running:
#    - Ollama: https://ollama.ai
#    - LM Studio: https://lmstudio.ai
#
# 5. For minimal testing, you only need one or two provider API keys